"""
Service layer for AI-powered consultation summaries.

Supports three providers controlled by settings.AI_PROVIDER:
  "openai"  — uses the OpenAI API (cloud)
  "ollama"  — uses a local Ollama instance via its OpenAI-compatible API
  "mock"    — returns a deterministic mocked summary (for dev / testing)

When AI_PROVIDER is *not* "mock", any provider error automatically falls
back to a mocked response so the endpoint never breaks.
"""

import logging

from django.conf import settings
from openai import APIConnectionError, APITimeoutError, AuthenticationError, OpenAI, RateLimitError

logger = logging.getLogger(__name__)


class AIServiceError(Exception):
    """Raised when the AI provider returns an unrecoverable error."""


# ── Mocked response ─────────────────────────────────────────────────
def _build_mock_summary(symptoms: str, diagnosis: str) -> str:
    """Return a deterministic, realistic-looking clinical summary."""
    return (
        f"**Chief Complaints:** {symptoms}\n\n"
        f"**Assessment:** {diagnosis}\n\n"
        f"**Summary:** The patient presents with the above symptoms, "
        f"which are consistent with a clinical assessment of {diagnosis}. "
        f"Further monitoring and follow-up are recommended to track "
        f"symptom progression and treatment response.\n\n"
        f"*— This summary was generated by the mock AI provider.*"
    )


# ── Provider resolution ─────────────────────────────────────────────
def _get_client_and_model() -> tuple[OpenAI, str]:
    """
    Build an OpenAI client (or Ollama-compatible client) and resolve the
    model name based on the active AI_PROVIDER setting.
    """
    provider = getattr(settings, "AI_PROVIDER", "openai").lower()

    if provider == "ollama":
        logger.info("Using Ollama provider at %s", settings.OLLAMA_BASE_URL)
        client = OpenAI(
            base_url=settings.OLLAMA_BASE_URL,
            api_key="ollama",  # Ollama ignores this, but the SDK requires it
        )
        model = settings.OLLAMA_MODEL
    else:
        # Default: OpenAI cloud
        if not settings.OPENAI_API_KEY:
            raise AIServiceError("OPENAI_API_KEY is not configured.")
        client = OpenAI(api_key=settings.OPENAI_API_KEY)
        model = settings.OPENAI_MODEL

    return client, model


# ── Public API ───────────────────────────────────────────────────────
def generate_consultation_summary(symptoms: str, diagnosis: str) -> str:
    """
    Send symptoms + diagnosis to the configured AI provider and return
    a structured clinical summary.

    If AI_PROVIDER is "mock", a deterministic mocked summary is returned
    immediately.  For "openai" / "ollama", any provider failure is caught
    and a mocked response is returned as a fallback (with a logged warning).
    """
    provider = getattr(settings, "AI_PROVIDER", "openai").lower()

    # ── Fast path: mock provider ─────────────────────────────────────
    if provider == "mock":
        logger.info("Using mock AI provider.")
        return _build_mock_summary(symptoms, diagnosis)

    # ── Real provider call ───────────────────────────────────────────
    try:
        client, model = _get_client_and_model()
    except AIServiceError:
        logger.warning("AI provider setup failed — falling back to mock response.")
        return _build_mock_summary(symptoms, diagnosis)

    system_prompt = (
        "You are a medical documentation assistant. "
        "Given a patient's symptoms and diagnosis, produce a concise, "
        "structured clinical summary in plain English. "
        "Include the following sections:\n"
        "1. **Chief Complaints** — a brief list of reported symptoms.\n"
        "2. **Assessment** — the diagnosis in clinical terms.\n"
        "3. **Summary** — a 2-3 sentence narrative tying symptoms to the diagnosis.\n"
        "Keep the output professional and suitable for medical records."
    )

    user_prompt = (
        f"Symptoms:\n{symptoms}\n\n"
        f"Diagnosis:\n{diagnosis}"
    )

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.3,
            max_tokens=512,
        )
        return response.choices[0].message.content.strip()

    except AuthenticationError:
        logger.error("AI authentication failed — falling back to mock response.")
    except RateLimitError:
        logger.warning("AI rate limit exceeded — falling back to mock response.")
    except (APIConnectionError, APITimeoutError):
        logger.error("AI connection/timeout error — falling back to mock response.")
    except Exception as exc:
        logger.exception("Unexpected AI error: %s — falling back to mock response.", exc)

    # Any exception above falls through here
    return _build_mock_summary(symptoms, diagnosis)
